{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Dask Icon](images/dask_horizontal_black.gif \"Dask Icon\")\n",
    "![Pandas Icon](images/pandas_logo.png \"Pandas Icon\")\n",
    "\n",
    "# Gotcha's from Pandas to Dask\n",
    "\n",
    "https://github.com/sephib/dask_pyconil2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This notebook highlights some key differences when transfering code from `Pandas` to run in a `Dask` environment.  \n",
    "Most issues have a link to the [Dask documentation](https://docs.dask.org/en/latest/) for additional information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Agenda  \n",
    "1. Intro to `Dask` framework\n",
    "2. Basic setup\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Dask Icon](images/dask_horizontal_black.gif \"Dask Icon\")\n",
    "\n",
    "Dask is a flexible library for parallel computing in Python.\n",
    "\n",
    "![Dask Framework](images/dask_graph_outline.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "Dask is composed of two parts:\n",
    "\n",
    "1. *Dynamic task scheduling* optimized for computation. This is similar to Airflow, Luigi, Celery, or Make, but optimized for interactive computational workloads.\n",
    "2. *“Big Data” collections* like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments. These parallel collections run on top of dynamic task schedulers.\n",
    "\n",
    "[link to documentation](https://docs.dask.org/en/latest/)\n",
    "\n",
    "Dask emphasizes the following virtues:\n",
    "\n",
    "* Familiar: Provides parallelized NumPy array and Pandas DataFrame objects\n",
    "* Flexible: Provides a task scheduling interface for more custom workloads and integration with other projects.\n",
    "* Native: Enables distributed computing in pure Python with access to the PyData stack.\n",
    "* Fast: Operates with low overhead, low latency, and minimal serialization necessary for fast numerical algorithms\n",
    "* Scales up: Runs resiliently on clusters with 1000s of cores\n",
    "* Scales down: Trivial to set up and run on a laptop in a single process\n",
    "* Responsive: Designed with interactive computing in mind, it provides rapid feedback and diagnostics to aid humans\n",
    "\n",
    "\n",
    "See the [dask.distributed documentation (separate website)](https://distributed.dask.org/en/latest/) for more technical information on Dask’s distributed scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask versoin: 1.2.2\n",
      "Pandas versoin: 0.24.2\n"
     ]
    }
   ],
   "source": [
    "# since Dask is activly beeing developed - the current example is running with the below version\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "print(f'Dask versoin: {dask.__version__}')\n",
    "print(f'Pandas versoin: {pd.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Start Dask Client for Dashboard\n",
    "![Dask Dashboard](images/dask_dashboard.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:54003\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>8.50 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:54003' processes=4 cores=4>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "# client = Client(n_workers=1, threads_per_worker=4, processes=False, memory_limit='2GB')\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Starting the Dask Client is optional.  In this example we are running on a `LocalCluster`, this  will also provide a dashboard which is useful to gain insight on the computation.  \n",
    "For additional information on [Dask Client see documentation](https://docs.dask.org/en/latest/setup.html?highlight=client#setup)  \n",
    "\n",
    "The link to the dashboard will become visible when you create a client (as shown below).  \n",
    "When running in `Jupyter Lab` an [extenstion](https://github.com/dask/dask-labextension) can be installed to be able to view the various dashboard widgets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "See [documentation for addtional cluster configuration](http://distributed.dask.org/en/latest/local-cluster.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* When running code within a script use `context manager`  \n",
    "see question in [stack overflow](https://stackoverflow.com/a/53520917/5817977)  \n",
    "* In order to get url dashboard use [inner function ](https://github.com/dask/distributed/issues/2083#issue-337057906)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python   \n",
    "from ... import ...\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with Client() as client:\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Create 2 DataFrames for comparison: \n",
    "* `Dask framework` is **lazy**  ![lazy python](images/Sleeping-snake.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dask.datasets.timeseries() #  Dask comes with builtin dataset samples, we will use this sample for our example. \n",
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to see the result we need to run [compute()](https://docs.dask.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.compute) \n",
    " (or `head()` which runs under the hood compute()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ddf.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pandas\n",
    "In order to create a `Pandas` dataframe we can use the `compute()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = ddf.compute()  \n",
    "print(type(pdf))\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Creating a `Dask dataframe` from `Pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In order to utilize `Dask` capablities on an existing `Pandas dataframe` (pdf) we need to convert the `Pandas dataframe` into a `Dask dataframe` (ddf)  with the [from_pandas](https://docs.dask.org/en/latest/dataframe-api.html#dask.dataframe.from_pandas) method. \n",
    "You must supply the number of `partitions` or `chunksize` that will be used to generate the dask dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf2 = dd.from_pandas(pdf, npartitions=10)\n",
    "print(type(ddf2))\n",
    "ddf2.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Partitions in Dask Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Notice that when we created a `Dask dataframe` we needed to supply an argument of `npartitions`.  \n",
    "The number of partitions will assist `Dask` on how it's going to parallelize the computation.  \n",
    "Each partition is a *separate* dataframe. For additional information see [partition documentation](https://docs.dask.org/en/latest/dataframe-design.html?highlight=meta%20utils#partitions)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Using `reset_index()` method we can examin the partitions:  \n",
    "First lets look at the `Pandas` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf2 = pdf.reset_index()\n",
    "# Only 1 row\n",
    "pdf2.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now lets look at a `Dask` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ddf2 = ddf2.reset_index() \n",
    "ddf2.loc[0].compute()  # each partition has an index=0\n",
    "# ddf2.loc[0].visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## dataframe.shape  \n",
    "since `Dask` is lazy we cannot get the full shape before running `len`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Pandas shape: {pdf.shape}')\n",
    "print('---------------------------')\n",
    "print(f'Dask lazy shape: {ddf.shape}') \n",
    "print(f'Dask computed shape: {len(ddf.index)}')  # expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now that we have a `dask` (ddf) and a `pandas` (pdf) dataframe we can start to compair the interactions with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conceptual shift - from Update to Insert/Delete\n",
    "\n",
    "\n",
    "![inplaceTrue](images/inplace_true.png \"inplace_true\")\n",
    "\n",
    "Dask does not update - thus there are no arguments such as `inplace=True` which exist in Pandas.  \n",
    "For more detials see [issue#653 on github](https://github.com/dask/dask/issues/653)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas \n",
    "print(pdf.columns)\n",
    "pdf.rename(columns={'id':'ID'}, inplace=True)\n",
    "# pdf = pdf.rename(columns={'id':'ID'})\n",
    "pdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* using `inplace=True` is not considerd to be *best practice*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Dask - Error\n",
    "# ddf.rename(columns={'id':'ID'}, inplace=True)\n",
    "# ddf.columns\n",
    "\n",
    "'''\n",
    "---------------------------------------------------------------------------  \n",
    "TypeError                                 Traceback (most recent call last)  \n",
    "<ipython-input-12-3e70ff3a549e> in <module>  \n",
    "      1 # Dask - Error  \n",
    "----> 2 ddf.rename(columns={'id':'ID'}, inplace=True)  \n",
    "      3 ddf.columns  \n",
    "TypeError: rename() got an unexpected keyword argument 'inplace'  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Dask or Pandas\n",
    "print(ddf.columns)\n",
    "ddf = ddf.rename(columns={'id':'ID'})\n",
    "ddf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data manipilations  \n",
    "There are several diffrences when manipulating data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loc - Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = (pdf['x']>0.5) & (pdf['x']<0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.loc[cond, ['y']] = pdf['y']* 100\n",
    "pdf[cond].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dask - use mask/where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error\n",
    "# cond_dask = (ddf['x']>0.5) & (ddf['x']<0.8)\n",
    "# ddf.loc[cond_dask, ['y']] = ddf['y']* 100\n",
    "\n",
    "'''\n",
    "> TypeError                                 Traceback (most recent call last)  \n",
    "> <ipython-input-16-2bbb2ae570bd> in <module> \n",
    ">       2 # Error  \n",
    "> ----> 3 ddf.loc[cond_dask, ['y']] = ddf['y']* 100  \n",
    "> TypeError: '_LocIndexer' object does not support item assignment  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_dask = (ddf['x']>0.5) & (ddf['x']<0.8)\n",
    "ddf['y'] = ddf['y'].mask(cond_dask, ddf['y']* 100)\n",
    "ddf[cond_dask].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[dask mask documentation](https://docs.dask.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Meta\n",
    "One key issue is the introduction of `meta` arguement.  \n",
    "> `meta` is the prescription of the names/types of the computation output   \n",
    "[see stack overflow answer](https://stackoverflow.com/questions/44432868/dask-dataframe-apply-meta)\n",
    "\n",
    "![crystal python](images/crystalBallsnake.png \"crystal snake\")\n",
    "Since `Dask` creates a DAG for the computation it requires to understand what are the outputs of each calculation (see [meta documentation](https://docs.dask.org/en/latest/dataframe-design.html?highlight=meta%20utils#metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pdf['initials'] = pdf['name'].apply(lambda x: x[0]+x[1])\n",
    "pdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ddf['initials'] = ddf['name'].apply(lambda x: x[0]+x[1])\n",
    "ddf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Describe the outcome type of the calculation\n",
    "meta_cal = pd.Series(object, name='initials')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf['initials'] = ddf['name'].apply(lambda x: x[0]+x[1]\n",
    "                                    , meta = meta_cal)\n",
    "ddf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def func(row, col1, col2):\n",
    "    if (row[col1]> 0):\n",
    "        return row[col1] * 1000  \n",
    "    else:\n",
    "        return row[col2] * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf['z'] = ddf.apply(func, args=('coor_x', 'coor_y'), axis=1, meta=('z', 'float'))\n",
    "ddf['z'] = ddf.apply(func,args=('x', 'y'), axis=1, meta=('z', 'float'))\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Map partitions\n",
    "* We can supply an ad-hoc function to run on each partition using the [map_partitions](https://dask.readthedocs.io/en/latest/dataframe-api.html#dask.dataframe.DataFrame.map_partitions) method.   \n",
    "Mainly useful for functions that are not implemented in `Dask` or `Pandas` . \n",
    "* Finally we can return a new `dataframe` which needs to be described in the `meta` argument  \n",
    "The function could also include arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def func2(df, coor_x, coor_y, drop_cols):\n",
    "    df['dist'] =  np.sqrt ( (df[coor_x] - df[coor_x].shift())**2  \n",
    "                           +  (df[coor_y] - df[coor_y].shift())**2 )\n",
    "    df = df.drop(drop_cols, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf2 = ddf.map_partitions(func2\n",
    "                          , coor_x='x'\n",
    "                          , coor_y='y'\n",
    "                          , drop_cols=['initials', 'z']\n",
    "                          , meta=pd.DataFrame({'ID':'i8'\n",
    "                                              , 'name':str\n",
    "                                              , 'x':'f8'\n",
    "                                              , 'y':'f8'                                              \n",
    "                                              , 'dist':'f8'}, index=[0]))\n",
    "ddf2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Convert index into DateTime column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only Pandas\n",
    "pdf = pdf.assign(times=pd.to_datetime(pdf.index).time)\n",
    "pdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# ddf = ddf.assign(Time= dask.dataframe.to_datetime(ddf.index, format='%Y-%m-%d'). )\n",
    "ddf = ddf.assign(times= dd.to_datetime(ddf.index).dt.time\n",
    "                , dates = dd.to_datetime(ddf.index).dt.date)                 \n",
    "ddf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Dask or Pandas\n",
    "ddf = ddf.assign(times=ddf.index.astype('M8[ns]'))\n",
    "ddf['dates'] = ddf['times'].dt.date\n",
    "ddf['times'] = ddf['times'].dt.time\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Drop NA on column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "pdf = pdf.assign(colna = None)\n",
    "print(pdf.head(2))\n",
    "pdf = pdf.dropna(axis=1, how='all')\n",
    "print(pdf.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In odrer for `Dask` to drop a column with all `na` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask\n",
    "ddf = ddf.assign(colna = None)\n",
    "# check if all values in column are Null - expensive\n",
    "if ddf.colna.isnull().all().compute() == True:   \n",
    "    ddf = ddf.drop(labels=['colna'],axis=1)\n",
    "print(ddf.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Reset Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "pdf = pdf.reset_index(drop=True)\n",
    "pdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dask\n",
    "ddf = ddf.reset_index()\n",
    "ddf = ddf.drop(labels=['timestamp'], axis=1 )\n",
    "ddf.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Read / Save files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with `pandas` and `dask` preferable try and work with [parquet](https://docs.dask.org/en/latest/dataframe-best-practices.html?highlight=parquet#store-data-in-apache-parquet-format).  \n",
    "Even so when working with `Dask` - the files can be read with multiple workers .  \n",
    "Most `kwargs` are applicable for reading and writing files [see documentaion](https://docs.dask.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.to_csv) (including the option for output file naming).  \n",
    "e.g. \n",
    "ddf = dd.read_csv('data/pd2dd/ddf*.csv', compression='gzip', header=False).  \n",
    "\n",
    "However some are not available such as  `nrows`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "from pathlib import Path\n",
    "output_file = 'pdf_single_file.csv'\n",
    "output_dir = Path('data/')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "pdf.to_csv(output_dir / output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(Path(output_dir).glob('*.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`Dask`\n",
    "Notice the '*' to allow for multiple file renaming. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dask_dir = Path('data/pd2dd/')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Dask\n",
    "ddf.to_csv(f'{output_dask_dir}/ddf*.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To find the number of partitions which will determine the number of output files use [dask.dataframe.npartitions](https://docs.dask.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.npartitions)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "list(Path(output_dask_dir).glob('*.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change the number of output files use [repartition](https://docs.dask.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.repartition) which is an expensive operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Read files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For `pandas` it is possible to iterate and concat the files [see answer from stack overflow](https://stackoverflow.com/questions/20906474/import-multiple-csv-files-into-pandas-and-concatenate-into-one-dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Pandas\n",
    "dir_path = Path(r'data/pd2dd')\n",
    "concat_df = pd.concat([pd.read_csv(f) for f in list(dir_path.glob('*.csv'))])\n",
    "len(concat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Dask\n",
    "_ddf = dd.read_csv('data/pd2dd/ddf*.csv')\n",
    "len(_ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " ## Consider using Persist\n",
    "Since Dask is lazy - it may run the **entire** graph/DAG (again) even if it already run part of the calculation in a previous cell.  Thus use [persist](https://docs.dask.org/en/latest/dataframe-best-practices.html?highlight=parquet#persist-intelligently) to keep the results in memory \n",
    "```python\n",
    "ddf = client.persist(ddf)\n",
    "```\n",
    "This is different from Pandas which once a variable was created it will keep all data in memory.  \n",
    "Additional information can be read in this [stackoverflow issue](https://stackoverflow.com/questions/45941528/how-to-efficiently-send-a-large-numpy-array-to-the-cluster-with-dask-array/45941529#45941529) or see an exampel in [this post](http://matthewrocklin.com/blog/work/2017/01/12/dask-dataframes)   \n",
    "This concept should also  be used when running a code within a script (rather then a jupyter notebook) which incoperates loops within the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ddf = dd.read_csv('data/pd2dd/ddf*.csv')\n",
    "ddf = client.persist(_ddf)\n",
    "ddf.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Group By - custom aggregations\n",
    "In addition to the [groupby notebook example](https://github.com/dask/dask-examples/blob/master/dataframes/02-groupby.ipynb)  - \n",
    "this is another example how to try to eliminate the use of `groupby.apply`  \n",
    "In this example we are grouping by columns into unique list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare pandas dataframe\n",
    "pdf = pdf.assign(Time=pd.to_datetime(pdf.index).time)\n",
    "pdf['seconds'] = pdf.Time.astype(str).str[-2:]\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pdf_gb = pdf.groupby(pdf.name)\n",
    "gp_col = ['ID', 'seconds']\n",
    "list_ser_gb = [pdf_gb[att_col_gr].apply\n",
    "               (lambda x: list(set(x.to_list()))) \n",
    "               for att_col_gr in gp_col]\n",
    "df_edge_att = pdf_gb.size().to_frame(name=\"Weight\")\n",
    "for ser in list_ser_gb:\n",
    "        df_edge_att = df_edge_att.join(ser.to_frame(), how='left')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edge_att.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In any case sometimes using Pandas is more efficiante (assuming that you can load all the data into the RAM).  \n",
    "In this case Pandas is faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def set_list_att(x: dd.Series):\n",
    "        return list(set([item for item in x.values]))\n",
    "ddf['seconds'] = ddf.times.astype(str).str[-2:]\n",
    "ddf = client.persist(ddf)\n",
    "ddf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Dask option1 using apply\n",
    "# notice the meta argument in the apply function\n",
    "df_gb = ddf.groupby(ddf.name)\n",
    "gp_col = ['ID', 'seconds']\n",
    "list_ser_gb = [df_gb[att_col_gr].apply(set_list_att\n",
    "                ,meta=pd.Series(dtype='object', name=f'{att_col_gr}_att')) \n",
    "               for att_col_gr in gp_col]\n",
    "df_edge_att = df_gb.size().to_frame(name=\"Weight\")\n",
    "for ser in list_ser_gb:\n",
    "    df_edge_att = df_edge_att.join(ser.to_frame(), how='left')\n",
    "df_edge_att.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using [dask custom aggregation](https://docs.dask.org/en/latest/dataframe-api.html?highlight=dropna#dask.dataframe.groupby.Aggregation) is consideribly better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask\n",
    "import itertools\n",
    "custom_agg = dd.Aggregation(\n",
    "    'custom_agg', \n",
    "    lambda s: s.apply(set), \n",
    "    lambda s: s.apply(lambda chunks: list(set(itertools.chain.from_iterable(chunks)))),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Dask option1 using apply\n",
    "df_gb = ddf.groupby(ddf.name)\n",
    "gp_col = ['ID', 'seconds']\n",
    "list_ser_gb = [df_gb[att_col_gr].agg(custom_agg) for att_col_gr in gp_col]\n",
    "df_edge_att = df_gb.size().to_frame(name=\"Weight\")\n",
    "for ser in list_ser_gb:\n",
    "        df_edge_att = df_edge_att.join(ser.to_frame(), how='left')\n",
    "df_edge_att.head(2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df_edge_att.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Debugging\n",
    "Debugging may be more challenging since\n",
    "1. when using a client - mutliprocessing is complecated\n",
    "2. sometime introducing a faulty command into a graph (such as in a jupyter notebook) requirues to cache-out the graph and start the process from the begining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Corrupted DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "ddf = dask.datasets.timeseries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns an error\n",
    "def func_dist2(df, coor_x, coor_y):\n",
    "    dist =  np.sqrt ( (df[coor_x] - df[coor_x].shift())^2  \n",
    "                     +  (df[coor_y] - df[coor_y].shift())^2 )\n",
    "    return dist\n",
    "ddf['col'] = ddf.map_partitions(func_dist2, coor_x='x', coor_y='y'\n",
    "                                , meta=('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Even if the function is currected the DAG is corrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still results with an error\n",
    "def func_dist2(df, coor_x, coor_y):\n",
    "    dist =  np.sqrt ( (df[coor_x] - df[coor_x].shift())**2  \n",
    "                     +  (df[coor_y] - df[coor_y].shift())**2 )\n",
    "    return dist\n",
    "ddf['col'] = ddf.map_partitions(func_dist2, coor_x='x', coor_y='y'\n",
    "                                , meta=('float'))\n",
    "ddf.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Need to reset the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dask.datasets.timeseries()\n",
    "def func_dist2(df, coor_x, coor_y):\n",
    "    dist =  np.sqrt ( (df[coor_x] - df[coor_x].shift())**2  +  (df[coor_y] - df[coor_y].shift())**2 )\n",
    "#     dist =  np.sqrt ( (df[coor_x] - df[coor_x].shift())^2  +  (df[coor_y] - df[coor_y].shift())^2 )\n",
    "    return dist\n",
    "ddf['col'] = ddf.map_partitions(func_dist2, coor_x='x', coor_y='y', meta=('float'))\n",
    "ddf.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "1. `Dask` is lazy but efficient (parallel computing)\n",
    "2. Usefull when comming from a `Pandas` (instead of `pysaprk`) \n",
    "3. Distributed environments - from single laptop to thousands clusters (including visabilty into the computation)\n",
    "4. But beware of:\n",
    "  * missing functionalities from `Pandas` API\n",
    "  * currupted DAGs\n",
    "\n",
    "https://github.com/sephib/dask_pyconil2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "rise": {
   "enable_chalkboard": true,
   "footer": "Gotcha's from Pandas to Dask - pyconil 2019"
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
